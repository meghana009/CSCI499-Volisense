{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import os\n","import yfinance as yf\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, TensorDataset\n","from transformers import BertTokenizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error\n","\n","text_directory_path = 'CSCI499/new_text'\n","\n","def aggregate_text(file_path):\n","  file_name = file_path.split('/')[-1]\n","  date_str = file_name.split('fdata')[1].split('_')[0]\n","  date = pd.to_datetime(date_str, format='%Y-%m-%d')\n","\n","  try:\n","    df = pd.read_csv(file_path)\n","    text = ' '.join(df['text'].astype(str))\n","    # text = summarization_pipeline(text, max_length=100, min_length=10, do_sample=False)[0]['summary_text']\n","    # print(\"\\n\", text)\n","    return {'Date': date, 'ConcatenatedText': text}\n","\n","  except:\n","    print('\\nOOPS')\n","    return {'Date': date, 'ConcatenatedText': None}\n","\n","count = 0\n","aggregated_data = []\n","print('Total files in new_text directory - ', len(os.listdir(text_directory_path)))\n","for file_name in os.listdir(text_directory_path):\n","  # if count > 20:\n","  #   break\n","  aggregated_data.append(aggregate_text(os.path.join(text_directory_path, file_name)))\n","  count += 1\n","\n","combined_df = pd.DataFrame(aggregated_data, columns=['Date', 'ConcatenatedText'])\n","combined_df.sort_values(by='Date', inplace=True)\n","combined_df.reset_index(drop=True, inplace=True)\n","# combined_df\n","\n","# Merging VIX data\n","vix_ticker = \"^VIX\"\n","vix = yf.Ticker(vix_ticker)\n","vix_data = vix.history(start=\"2019-01-01\", end=\"2022-12-31\")\n","vix_data.reset_index(inplace=True)\n","vix_data['Date'] = pd.to_datetime(vix_data['Date'], format='%Y-%m-%d').dt.tz_localize(None)\n","combined_df = pd.merge(combined_df, vix_data[['Date', 'Close']], on='Date', how='inner')\n","combined_df.rename(columns={'Close': 'VIX_Close'}, inplace=True)\n","print(combined_df)\n","\n","from transformers import AutoModelForSequenceClassification\n","model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n","                                                           num_labels=1)\n","\n","null_values = combined_df['ConcatenatedText'].isnull().sum()\n","print(null_values)\n","combined_df['ConcatenatedText'].fillna('', inplace=True)\n","null_values = combined_df['ConcatenatedText'].isnull().sum()\n","print(null_values)\n","\n","texts = combined_df['ConcatenatedText'].tolist()\n","vix_values = (np.log(combined_df['VIX_Close'])).tolist()\n","\n","classes = []\n","for j in range(len(vix_values)):\n","  if vix_values[j] >= 6.47 and vix_values[j] < 12.95:\n","    classes.append('Low')\n","  elif vix_values[j] > 12.95 and vix_values[j] < 18.32:\n","    classes.append('Moderate')\n","  elif vix_values[j] > 18.32 and vix_values[j] < 36.68:\n","    classes.append('High')\n","  else:\n","    classes.append('Extreme')\n","\n","train_articles, val_articles, train_vix, val_vix = train_test_split(texts, vix_values, test_size=0.2, random_state=42)\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","train_encodings = tokenizer(train_articles, truncation=True, padding=True, max_length=512)\n","val_encodings = tokenizer(val_articles, truncation=True, padding=True, max_length=512)\n","\n","train_dataset = torch.utils.data.TensorDataset(torch.tensor(train_encodings['input_ids']), torch.tensor(train_encodings['attention_mask']), torch.tensor(train_vix))\n","val_dataset = torch.utils.data.TensorDataset(torch.tensor(val_encodings['input_ids']), torch.tensor(val_encodings['attention_mask']), torch.tensor(val_vix))\n","\n","train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n","val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=True)\n","\n","train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n","val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n","\n","#model = MyBERTRegressionModel(input_size=len(tokenizer), hidden_size=768)\n","model = model.float()\n","criterion = nn.MSELoss()\n","# criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n","\n","import logging\n","\n","logging.basicConfig(level=logging.INFO)\n","\n","num_epochs = 3\n","for epoch in range(num_epochs):\n","    model.train()\n","    for batch in train_dataloader:\n","        input_ids, attention_mask, target = batch\n","\n","        optimizer.zero_grad()\n","        #model = model.float()\n","\n","        outputs = model(input_ids, attention_mask, labels=target)\n","        #outputs = model(input_ids, attention_mask)\n","        # loss = criterion(outputs.to(torch.float32), target.unsqueeze(1)).to(torch.float32)\n","        #print(type(loss))\n","        #loss = criterion(outputs, target)\n","        loss = outputs.loss\n","        #loss = criterion(outputs.logits, target)\n","        loss.backward()\n","        optimizer.step()\n","\n","          #print(f'Epoch [Loss: {loss.item():.4f}')\n","    model.eval()\n","    with torch.no_grad():\n","      val_loss = 0\n","      for batch in val_dataloader:\n","        input_ids,attention_mask,target = batch\n","        #outputs = model(input_ids, attention_mask).to(torch.float32)\n","        outputs = model(input_ids, attention_mask, labels=target)\n","        #outputs = model(input_ids, attention_mask)\n","        #loss = criterion(outputs.to(torch.float32), target.unsqueeze(1)).to(torch.float32)\n","        loss = outputs.loss\n","        #loss = criterion(outputs, target)\n","        val_loss += loss.item()\n","    avg_val_loss = val_loss/len(val_dataloader)\n","    print(f'Epoch{epoch}[Validation Loss:{avg_val_loss}]')\n","# Save the trained model\n","torch.save(model.state_dict(), '.bert_regression_model.pth')\n","\n","model.eval()  # Set the model to evaluation mode\n","total_mse = 0\n","num_samples = 0\n","\n","with torch.no_grad():\n","    for batch in val_dataloader:\n","        input_ids, attention_mask, target = batch\n","\n","        outputs = model(input_ids, attention_mask, labels=target)\n","        loss = outputs.loss  # Calculate loss\n","        total_mse += loss.item()  # Accumulate loss\n","        num_samples += len(input_ids)  # Update the total number of samples\n","\n","# Calculate average MSE\n","avg_mse = total_mse / len(val_dataloader)\n","\n","print(f\"Validation MSE: {avg_mse}\")\n"],"metadata":{"id":"zEA0rtfEUROe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"YuMFGAju0pGX"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iYjgQEZC_skC"},"outputs":[],"source":["import pandas as pd\n","import os\n","import yfinance as yf\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, TensorDataset\n","from transformers import BertTokenizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error"]},{"cell_type":"code","source":["text_directory_path = '/content/drive/MyDrive/CSCI-499 Project/new_text'"],"metadata":{"id":"mZ5LIKwhAvcX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def aggregate_text(file_path):\n","  file_name = file_path.split('/')[-1]\n","  date_str = file_name.split('fdata')[1].split('_')[0]\n","  date = pd.to_datetime(date_str, format='%Y-%m-%d')\n","\n","  try:\n","    df = pd.read_csv(file_path)\n","    text = ' '.join(df['text'].astype(str))\n","    # text = summarization_pipeline(text, max_length=100, min_length=10, do_sample=False)[0]['summary_text']\n","    # print(\"\\n\", text)\n","    return {'Date': date, 'ConcatenatedText': text}\n","\n","  except:\n","    print('\\nOOPS')\n","    return {'Date': date, 'ConcatenatedText': None}"],"metadata":{"id":"x7Ep4SoCAwFB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["count = 0\n","aggregated_data = []\n","print('Total files in new_text directory - ', len(os.listdir(text_directory_path)))\n","for file_name in os.listdir(text_directory_path):\n","  # if count > 20:\n","  #   break\n","  aggregated_data.append(aggregate_text(os.path.join(text_directory_path, file_name)))\n","  count += 1\n","\n","combined_df = pd.DataFrame(aggregated_data, columns=['Date', 'ConcatenatedText'])\n","combined_df.sort_values(by='Date', inplace=True)\n","combined_df.reset_index(drop=True, inplace=True)\n","# combined_df"],"metadata":{"id":"4Vi8F3QbAyRR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Merging VIX data\n","vix_ticker = \"^VIX\"\n","vix = yf.Ticker(vix_ticker)\n","vix_data = vix.history(start=\"2019-01-01\", end=\"2022-12-31\")\n","vix_data.reset_index(inplace=True)\n","vix_data['Date'] = pd.to_datetime(vix_data['Date'], format='%Y-%m-%d').dt.tz_localize(None)\n","combined_df = pd.merge(combined_df, vix_data[['Date', 'Close']], on='Date', how='inner')\n","combined_df.rename(columns={'Close': 'VIX_Close'}, inplace=True)\n","print(combined_df)"],"metadata":{"id":"9BiHvHw6A0Zr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# class MyBERTRegressionModel(nn.Module):\n","#     def __init__(self, input_size, hidden_size=768):\n","#         super(MyBERTRegressionModel, self).__init__()\n","#         self.embedding = nn.Embedding(input_size, hidden_size)\n","#         self.linear = nn.Linear(hidden_size, 1)\n","\n","#     def forward(self, input_ids, attention_mask):\n","#         embedded = self.embedding(input_ids)\n","#         output = self.linear(embedded.sum(dim=1))\n","#         return output"],"metadata":{"id":"Wyy6Cu-bBITb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from transformers import BertTokenizer, BertModel\n","# model = BertModel.from_pretrained('bert-base-uncased')"],"metadata":{"id":"dP80yKlQM8Np"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoModelForSequenceClassification\n","model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n","                                                           num_labels=1)"],"metadata":{"id":"3VYGN-MCO2ne"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["null_values = combined_df['ConcatenatedText'].isnull().sum()\n","print(null_values)\n","combined_df['ConcatenatedText'].fillna('', inplace=True)\n","null_values = combined_df['ConcatenatedText'].isnull().sum()\n","print(null_values)"],"metadata":{"id":"pdXqLGg1XLy2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["texts = combined_df['ConcatenatedText'].tolist()\n","vix_values = (np.log(combined_df['VIX_Close']).tolist()"],"metadata":{"id":"cSvsp35vg8hi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["classes = []\n","for j in range(len(vix_values)):\n","  if vix_values[j] >= 6.47 and vix_values[j] < 12.95:\n","    classes.append('Low')\n","  elif vix_values[j] > 12.95 and vix_values[j] < 18.32:\n","    classes.append('Moderate')\n","  elif vix_values[j] > 18.32 and vix_values[j] < 36.68:\n","    classes.append('High')\n","  else:\n","    classes.append('Extreme')"],"metadata":{"id":"d-DeJadrqUm8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["classes"],"metadata":{"id":"p_ZA2b8Frdoa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_articles, val_articles, train_vix, val_vix, train_labels, val_labels = train_test_split(texts, vix_values, classes, test_size=0.2, random_state=42)"],"metadata":{"id":"ygfGhLVD_fjg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","train_encodings = tokenizer(train_articles, truncation=True, padding=True, max_length=512)\n","val_encodings = tokenizer(val_articles, truncation=True, padding=True, max_length=512)\n"],"metadata":{"id":"ODk3CFht_a1C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train_labels = [classes[vix_values.index(value)] for value in train_vix]\n","# val_labels = [classes[vix_values.index(value)] for value in val_vix]"],"metadata":{"id":"58mN4Ujb080u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(train_labels)\n","#torch.tensor(train_encodings['attention_mask']).shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FqsEOsn-1SaR","executionInfo":{"status":"ok","timestamp":1713303464490,"user_tz":420,"elapsed":160,"user":{"displayName":"Venkata Meghana Achanta","userId":"15385269140052891818"}},"outputId":"839a540f-8055-43c9-d2d9-d16cd3a84dd9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["307"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["# from sklearn.preprocessing import LabelEncoder\n","# label_encoder = LabelEncoder()\n","# train_labels_encoded = label_encoder.fit_transform(train_labels)\n","# val_labels_encoded = label_encoder.fit_transform(val_labels)"],"metadata":{"id":"E_Gr1fXg2X9U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset = TensorDataset(torch.tensor(train_encodings['input_ids']),\n","                              torch.tensor(train_encodings['attention_mask']),\n","                              torch.tensor(train_labels_encoded))\n","\n","val_dataset = TensorDataset(torch.tensor(val_encodings['input_ids']),\n","                            torch.tensor(val_encodings['attention_mask']),\n","                            torch.tensor(val_labels_encoded))\n"],"metadata":{"id":"VXDCNM7pw2Cz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n","val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n"],"metadata":{"id":"XOKgOfkRxBf3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train_dataset = torch.utils.data.TensorDataset(torch.tensor(train_encodings['input_ids']), torch.tensor(train_encodings['attention_mask']), torch.tensor(train_vix))\n","# val_dataset = torch.utils.data.TensorDataset(torch.tensor(val_encodings['input_ids']), torch.tensor(val_encodings['attention_mask']), torch.tensor(val_vix))\n","\n","# train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n","# val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=True)"],"metadata":{"id":"MU97bqNABOsH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#model = MyBERTRegressionModel(input_size=len(tokenizer), hidden_size=768)\n","#model = model.float()\n","#criterion = nn.MSELoss()\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)"],"metadata":{"id":"f5LdG6FABRQs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import logging\n","\n","logging.basicConfig(level=logging.INFO)"],"metadata":{"id":"AsaYz_XDBU6W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_epochs = 3\n","for epoch in range(num_epochs):\n","    model.train()\n","    for batch in train_dataloader:\n","        input_ids, attention_mask, target = batch\n","\n","        optimizer.zero_grad()\n","        #model = model.float()\n","\n","        #outputs = model(input_ids, attention_mask, labels=target)\n","        outputs = model(input_ids, attention_mask)\n","        # loss = criterion(outputs.to(torch.float32), target.unsqueeze(1)).to(torch.float32)\n","        #print(type(loss))\n","        #loss = criterion(outputs, target)\n","        #loss = outputs.loss\n","        loss = criterion(outputs.logits, target)\n","        loss.backward()\n","        optimizer.step()\n","\n","          #print(f'Epoch [Loss: {loss.item():.4f}')\n","    model.eval()\n","    with torch.no_grad():\n","      val_loss = 0\n","      for batch in val_dataloader:\n","        input_ids,attention_mask,target = batch\n","        #outputs = model(input_ids, attention_mask).to(torch.float32)\n","        #outputs = model(input_ids, attention_mask, labels=target)\n","        outputs = model(input_ids, attention_mask)\n","        #loss = criterion(outputs.to(torch.float32), target.unsqueeze(1)).to(torch.float32)\n","        #loss = outputs.loss\n","        loss = criterion(outputs, target)\n","        val_loss += loss.item()\n","    avg_val_loss = val_loss/len(val_dataloader)\n","    print(f'Epoch{epoch}[Validation Loss:{avg_val_loss}]')\n","# Save the trained model\n","torch.save(model.state_dict(), '.bert_regression_model.pth')"],"metadata":{"id":"0WCa2O3XBXG0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.eval()  # Set the model to evaluation mode\n","total_mse = 0\n","num_samples = 0\n","\n","with torch.no_grad():\n","    for batch in val_dataloader:\n","        input_ids, attention_mask, target = batch\n","\n","        outputs = model(input_ids, attention_mask, labels=target)\n","        loss = outputs.loss  # Calculate loss\n","        total_mse += loss.item()  # Accumulate loss\n","        num_samples += len(input_ids)  # Update the total number of samples\n","\n","# Calculate average MSE\n","avg_mse = total_mse / len(val_dataloader)\n","\n","print(f\"Validation MSE: {avg_mse}\")\n"],"metadata":{"id":"pa1IEgyTfq1Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["1. Validation MSE: 423.5490132780636 </br>\n","Learning Rate = 1e-5 </br>\n","train batch_size = 8 </br>\n","val batch size = 2 </br>\n","\n","\n","2. Validation MSE: 311.59172973632815 </br>\n","Learning Rate = 1e-4 </br>\n","train batch_size = 8 </br>\n","val batch size = 8 </br>\n","\n","\n","3. Validation MSE: 275.0124104817708 </br>\n","Learning Rate = 1e-3 </br>\n","train batch_size = 16 </br>\n","val batch size = 16 </br>\n","\n","4. Validation MSE: 4196.583740234375 </br>\n","Learning Rate = 1e-6 </br>\n","train batch_size = 32 </br>\n","val batch size = 32 </br>\n","\n","4. Validation MSE: 394.6639862060547 </br>\n","Learning Rate = 1e-5 </br>\n","train batch_size = 16 </br>\n","val batch size = 16 </br>\n","\n"],"metadata":{"id":"AJr3ZLeW3-4A"}},{"cell_type":"markdown","source":["**bold text** New Readings </br>\n","\n","1. Validation MSE: 34.17598762512207 </br>\n","Learning Rate = 1e-4 </br>\n","train batch_size = 8 </br>\n","val batch size = 8 </br>\n","\n","2."],"metadata":{"id":"qMx1q4noxmy1"}},{"cell_type":"code","source":["# vix_data.reset_index(inplace=True)"],"metadata":{"id":"ceKRuWD_DQEl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# vix_data"],"metadata":{"id":"6B87ZSVjDcIK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from google.colab import drive\n","# drive.mount('/content/drive')"],"metadata":{"id":"OqTn-baNRRDM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# vix_data.index"],"metadata":{"id":"JXtY69vYFw5O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print(vix_data.set_index(['Date'],inplace=True))"],"metadata":{"id":"JFnXWRVXE8CE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# (vix_data.index)"],"metadata":{"id":"p35okcSHLnNp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import os\n","# import pandas as pd\n","\n","# #NEW CODE ADDED BY VAIBHAV\n","\n","# # Define directory containing CSV files\n","# #directory = '/content/drive/My Drive/CSCI-499 Project/text'\n","# directory = '/content/text'\n","\n","# # Initialize an empty DataFrame to store the final data\n","# #final_df = pd.DataFrame(columns=['Date', 'ConcatenatedText'])\n","# res = []\n","# # Iterate over CSV files in the directory\n","# for filename in os.listdir(directory):\n","#     if filename.endswith('.csv'):\n","#         # Extract the date from the filename\n","#         #date_str = filename.split('_')[1].split('.')[0]\n","#         date_str = filename.split('fdata')[1].split('.')[0]\n","#         date = pd.to_datetime(date_str, format='%Y-%m-%d')\n","\n","\n","#         # Construct full file path\n","#         filepath = os.path.join(directory, filename)\n","\n","#         # Load CSV file into a DataFrame\n","#         df = pd.read_csv(filepath)\n","\n","#         # Concatenate all rows into a single string separated by the pound symbol\n","#         concatenated_text = '#'.join(df.iloc[:, 0].astype(str).tolist())\n","#         new_row = {'Date': [date], 'ConcatenatedText': [concatenated_text]}\n","#         # Append the date and concatenated text to the final DataFrame\n","#         #final_df = final_df.append(new_row, ignore_index=True)\n","#         res.append(new_row)\n","# final_df = pd.DataFrame(res, columns=['Date', 'ConcatenatedText'])\n","# # Sort the final DataFrame by date\n","# #final_df['Date'] = pd.to_datetime(final_df['Date'])\n","# final_df.sort_values(by='Date', inplace=True)\n","\n","# # Reset the index of the final DataFrame\n","# final_df.reset_index(drop=True, inplace=True)\n","\n","# import yfinance as yf\n","# import pandas as pd\n","# import os\n","\n","# # Define the VIX ticker symbol\n","# vix_ticker = \"^VIX\"\n","\n","# # Create a Ticker object for the VIX\n","# vix = yf.Ticker(vix_ticker)\n","\n","# # Fetch historical data for the VIX\n","# vix_data = vix.history(start=\"2019-01-01\", end=\"2020-03-02\")\n","\n","# # Reset the index of vix_data to convert the Date index into a column\n","# vix_data.reset_index(inplace=True)\n","\n","# # Ensure the date format matches that of final_df and set it as the 'Date' column\n","# vix_data['Date'] = pd.to_datetime(vix_data['Date'], format='%Y-%m-%d')\n","\n","# # Now merge the two DataFrames on the 'Date' column using an inner join\n","# # This will only keep rows that have matching dates in both DataFrames\n","# #final_df['Date'] = pd.to_datetime(final_df['Date'])\n","# final_df = pd.merge(final_df, vix_data[['Date', 'Close']], on='Date', how='inner')\n","# #final_df = pd.concat(final_df, vix_data[['Date', 'Close']], on='Date', how='inner')\n","# # Rename the 'Close' column to something more descriptive\n","# final_df.rename(columns={'Close': 'VIX_Close'}, inplace=True)\n","\n","# # Your final_df now has an additional column 'VIX_Close' and only contains rows\n","# # for which both text data and VIX data are available.\n","\n"],"metadata":{"id":"v7A7UFRbBKaQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import os\n","# import pandas as pd\n","\n","# # Define directory containing CSV files\n","# directory = '/content/text'\n","\n","# # Initialize an empty list to store dictionaries\n","# data_list = []\n","\n","# # Iterate over CSV files in the directory\n","# for filename in os.listdir(directory):\n","#     if filename.endswith('.csv'):\n","#         # Extract the date from the filename\n","#         date_str = filename.split('fdata')[1].split('.')[0]\n","#         date = pd.to_datetime(date_str, format='%Y-%m-%d')\n","\n","#         # Construct full file path\n","#         filepath = os.path.join(directory, filename)\n","\n","#         # Load CSV file into a DataFrame\n","#         df = pd.read_csv(filepath)\n","\n","#         # Concatenate all rows into a single string separated by the pound symbol\n","#         concatenated_text = '#'.join(df.iloc[:, 0].astype(str).tolist())\n","\n","#         # Append the date and concatenated text to the list\n","#         data_list.append({'Date': date, 'ConcatenatedText': concatenated_text})\n","\n","# # Convert the list of dictionaries to a DataFrame\n","# final_df = pd.DataFrame(data_list)\n","\n","# # Convert the 'Date' column to datetime\n","# final_df['Date'] = pd.to_datetime(final_df['Date'])\n","\n","# # Sort the final DataFrame by date\n","# final_df.sort_values(by='Date', inplace=True)\n","\n","# # Reset the index of the final DataFrame\n","# final_df.reset_index(drop=True, inplace=True)\n","\n","# import yfinance as yf\n","\n","# # Define the VIX ticker symbol\n","# vix_ticker = \"^VIX\"\n","\n","# # Create a Ticker object for the VIX\n","# vix = yf.Ticker(vix_ticker)\n","\n","# # Fetch historical data for the VIX\n","# vix_data = vix.history(start=\"2019-01-01\", end=\"2019-05-30\")\n","\n","# # Reset the index of vix_data to convert the Date index into a column\n","# vix_data.reset_index(inplace=True)\n","\n","# # Ensure the date format matches that of final_df and set it as the 'Date' column\n","# vix_data['Date'] = pd.to_datetime(vix_data['Date'], format='%Y-%m-%d')\n","\n","# # Now merge the two DataFrames on the 'Date' column using an inner join\n","# # This will only keep rows that have matching dates in both DataFrames\n","# # final_df = pd.merge(final_df, vix_data[['Date', 'Close']], on='Date', how='inner')\n","\n","# # # Rename the 'Close' column to something more descriptive\n","# # final_df.rename(columns={'Close': 'VIX_Close'}, inplace=True)\n","\n","# # Concatenate final_df and vix_data vertically using pd.concat()\n","# final_df = pd.concat([final_df, vix_data[['Date', 'Close']]], axis=0)\n","\n","# # Reset the index of the final DataFrame\n","# final_df.reset_index(drop=True, inplace=True)\n","\n","# # Rename the 'Close' column to something more descriptive\n","# final_df.rename(columns={'Close': 'VIX_Close'}, inplace=True)\n","\n","\n","# # Your final_df now has an additional column 'VIX_Close' and only contains rows\n","# # for which both text data and VIX data are available.\n"],"metadata":{"id":"TbPK5qkTfiG0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# vix_data['Close'].head() #treat the closing value for the day as the vix for the day"],"metadata":{"id":"KRWlEU6xRzGg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import yfinance as yf\n","# import pandas as pd\n","\n","# # Define the VIX ticker symbol\n","# vix_ticker = \"^VIX\"\n","\n","# # Create a Ticker object for the VIX\n","# vix = yf.Ticker(vix_ticker)\n","\n","# # Fetch historical data for the VIX from its earliest available date to the present\n","# vix_data = vix.history(start=\"2019-01-01\", end=\"2019-05-29\")"],"metadata":{"id":"pSdz3PUfRmDj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# final_df"],"metadata":{"id":"J0T42Qw9d8Lm"},"execution_count":null,"outputs":[]}]}